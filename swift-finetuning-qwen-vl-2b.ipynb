{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9385819,"sourceType":"datasetVersion","datasetId":5694652},{"sourceId":9385993,"sourceType":"datasetVersion","datasetId":5694787},{"sourceId":9396232,"sourceType":"datasetVersion","datasetId":5702884},{"sourceId":9396765,"sourceType":"datasetVersion","datasetId":5698110}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# install all the requirements for miniCPM-V\n!pip install git+https://github.com/huggingface/transformers\n!pip install --upgrade modelscope sentencepiece accelerate bitsandbytes datamodel_code_generator","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install deepspeed>=0.9.3\n!pip install liger-kernel-nightly==0.2.1.dev20240911164559","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/modelscope/swift.git\n%cd swift\n!pip install -e '.[llm]'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyav","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install qwen_vl_utils","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## QWEN-VL-2B DATASET CREATION","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/input/amazon-ml-train/train.csv')\n\n# Calculate the sample size (1% of total rows)\nsample_size = int(len(df) * 0.01)\n\n# Perform stratified sampling\nstratified_sample, _ = train_test_split(\n    df,\n    test_size=1 - (sample_size / len(df)),\n    stratify=df['entity_name'],\n    random_state=42  # for reproducibility\n)\n\n# Verify the sample size\nprint(f\"Original dataset size: {len(df)}\")\nprint(f\"Sampled dataset size: {len(stratified_sample)}\")\n\n# Verify the distribution\nprint(\"\\nOriginal distribution:\")\nprint(df['entity_name'].value_counts(normalize=True))\nprint(\"\\nSampled distribution:\")\nprint(stratified_sample['entity_name'].value_counts(normalize=True))\n\n# Save the sampled dataset\nstratified_sample.to_csv('/kaggle/working/sample_train.csv', index=False)\nprint(\"\\nSampled dataset saved as 'sample_train.csv'\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ..","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download images\nimport os\nfrom time import time\nimport multiprocessing\nimport logging\nimport requests\nimport pandas as pd\nimport time as t\nimport random \n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nimg_dir = 'image_dir'\n\ndef get_sbu_urls():\n    df = pd.read_csv('/kaggle/working/sample_train.csv')\n    urls = df['image_link'].tolist()\n    indices = df.index\n    return list(zip(indices, urls))\n\ndef scrape_and_save(args):\n    url, savepath = args\n    retries = 3\n    for attempt in range(retries):\n        try:\n            session = requests.Session()\n            response = session.get(url, timeout=1)\n            response.raise_for_status()\n            with open(savepath, 'wb') as f:\n                f.write(response.content)\n            # logging.info(f\"Successfully downloaded: {url}\")\n            return True\n        except requests.RequestException as e:\n            pass\n            # logging.error(f\"Error downloading {url}: {e} (Attempt {attempt+1}/{retries})\")\n        except IOError as e:\n            # logging.error(f\"Error saving file {savepath}: {e}\")\n            return False\n        t.sleep(2)  # small delay before retrying\n    return False\n\nif __name__ == '__main__':\n    startidx = 0\n    chunk_size = 5000  # Break downloads into chunks\n    urls = get_sbu_urls()\n    total_urls = len(urls)\n\n    if not os.path.exists(img_dir):\n        os.makedirs(img_dir)\n\n    starttime = time()\n    piccounter = 0\n\n    # Download in chunks\n    for chunk_start in range(startidx, total_urls, chunk_size):\n        chunk_end = min(chunk_start + chunk_size, total_urls)\n        url_chunk = urls[chunk_start:chunk_end]\n        \n        pool = multiprocessing.Pool(16)  # create a new pool for each chunk\n\n        results = []\n        for i, (index, url) in enumerate(url_chunk, start=chunk_start):\n            name = f'train_{index}.jpg'\n            savepath = os.path.join(img_dir, name)\n            result = pool.apply_async(scrape_and_save, ((url, savepath),))\n            results.append(result)\n\n        pool.close()  # no more tasks\n        pool.join()  # wait for completion\n\n        # Count successful downloads in the chunk\n        successful_downloads = sum([r.get() for r in results])\n        piccounter += successful_downloads\n\n        logging.info(f\"Downloaded {successful_downloads}/{chunk_size} in chunk {chunk_start//chunk_size + 1}. Total downloaded: {piccounter}/{total_urls}\")\n\n        # Throttle requests to prevent server throttling (optional)\n        t.sleep(10)  # 10-second break between chunks\n\n    print(f\"Downloaded {piccounter}/{total_urls} images.\")\n    print(f\"Total time taken: {time() - starttime:.2f} seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os\n\ndef prepare_dataset(csv_path, image_dir, output_json_path, question, answer_column):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n    \n    # Prepare the data in the required format\n    data = []\n    for index, row in df.iterrows():\n        image_path = os.path.join(image_dir, f\"train_{index}.jpg\")\n        \n        # Check if the image file exists\n        if not os.path.exists(image_path):\n            print(f\"Warning: Image file not found for index {index}\")\n            continue\n        \n        entry = {\n            \"system\": \"You are an entity extractor OCR model. Given the entity, you can extract the text from the image denoting the entity value. Entity value is always a number.\",\n            \"query\": f\"<image>What is the value of the {row['entity_name']} of the item shown? Give me numerical value as seen in the image. Adhere to instructions.\",\n            \"response\": f\"{row['entity_value'].split()[0]}\",\n            \"images\": [f\"{image_path}\"]\n        }\n        data.append(entry)\n    \n    # Write the data to a JSON file\n    with open(output_json_path, 'w') as f:\n        json.dump(data, f, indent=2)\n    \n    print(f\"Dataset prepared and saved to {output_json_path}\")\n\n# Usage\ncsv_path = '/kaggle/working/sample_train.csv'\nimage_dir = '/kaggle/working/image_dir/'\noutput_json_path = '/kaggle/working/train.json'\nquestion = \"What is the value of the {row['entity_name']} of the item shown? Give me numerical value as seen in the image. Adhere to instructions.\"\nanswer_column = 'entity_value'\n\nprepare_dataset(csv_path, image_dir, output_json_path, question, answer_column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Single-card A10/3090 can run\n# GPU Memory: 20GB\n!SIZE_FACTOR=8 MAX_PIXELS=602112 CUDA_VISIBLE_DEVICES=0,1 swift sft \\\n    --model_type qwen2-vl-2b-instruct \\\n    --model_id_or_path qwen/Qwen2-VL-2B-Instruct \\\n    --sft_type lora \\\n    --dataset /kaggle/working/train.json \\\n    --freeze_vit \\\n    --seed 42 \\\n    --max_length 128 \\\n    --lora_rank 32 \\\n    --use_liger \\\n    --num_train_epochs 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r checkpoint163.zip /kaggle/working/output/qwen2-vl-2b-instruct/v3-20240914-130231/checkpoint-163/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls -lah","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## INFERENCE","metadata":{}},{"cell_type":"code","source":"# download images\nimport os\nfrom time import time\nimport multiprocessing\nimport logging\nimport requests\nimport pandas as pd\nimport time as t\nimport random \n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nimg_dir = 'sample_test_image_dir'\n\ndef get_sbu_urls():\n    df = pd.read_csv('/kaggle/input/sample-test-amc/sample_test.csv')\n    urls = df['image_link'].tolist()\n    indices = df.index\n    return list(zip(indices, urls))\n\ndef scrape_and_save(args):\n    url, savepath = args\n    retries = 3\n    for attempt in range(retries):\n        try:\n            session = requests.Session()\n            response = session.get(url, timeout=1)\n            response.raise_for_status()\n            with open(savepath, 'wb') as f:\n                f.write(response.content)\n            # logging.info(f\"Successfully downloaded: {url}\")\n            return True\n        except requests.RequestException as e:\n            pass\n            # logging.error(f\"Error downloading {url}: {e} (Attempt {attempt+1}/{retries})\")\n        except IOError as e:\n            # logging.error(f\"Error saving file {savepath}: {e}\")\n            return False\n        t.sleep(2)  # small delay before retrying\n    return False\n\nif __name__ == '__main__':\n    startidx = 0\n    chunk_size = 5000  # Break downloads into chunks\n    urls = get_sbu_urls()\n    total_urls = len(urls)\n\n    if not os.path.exists(img_dir):\n        os.makedirs(img_dir)\n\n    starttime = time()\n    piccounter = 0\n\n    # Download in chunks\n    for chunk_start in range(startidx, total_urls, chunk_size):\n        chunk_end = min(chunk_start + chunk_size, total_urls)\n        url_chunk = urls[chunk_start:chunk_end]\n        \n        pool = multiprocessing.Pool(16)  # create a new pool for each chunk\n\n        results = []\n        for i, (index, url) in enumerate(url_chunk, start=chunk_start):\n            name = f'test_{index}.jpg'\n            savepath = os.path.join(img_dir, name)\n            result = pool.apply_async(scrape_and_save, ((url, savepath),))\n            results.append(result)\n\n        pool.close()  # no more tasks\n#         pool.join()  # wait for completion\n\n        # Count successful downloads in the chunk\n        successful_downloads = sum([r.get() for r in results])\n        piccounter += successful_downloads\n\n        logging.info(f\"Downloaded {successful_downloads}/{chunk_size} in chunk {chunk_start//chunk_size + 1}. Total downloaded: {piccounter}/{total_urls}\")\n\n        # Throttle requests to prevent server throttling (optional)\n        t.sleep(10)  # 10-second break between chunks\n\n    print(f\"Downloaded {piccounter}/{total_urls} images.\")\n    print(f\"Total time taken: {time() - starttime:.2f} seconds\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os\n\ndef prepare_dataset(csv_path, image_dir, output_json_path, question, answer_column):\n    # Read the CSV file\n    df = pd.read_csv(csv_path)\n    df = df[:10000]\n    # Prepare the data in the required format\n    data = []\n    for index, row in df.iterrows():\n        image_path = os.path.join(image_dir, f\"test_{index}.jpg\")\n        \n        # Check if the image file exists\n        if not os.path.exists(image_path):\n#             print(f\"Warning: Image file not found for index {index}\")\n            continue\n        \n        entry = {\n            \"system\": \"You are an entity extractor OCR model. Given the entity, you can extract the text from the image denoting the entity value. Entity value is always a number.\",\n            \"query\": f\"<image>What is the value of the {row['entity_name']} of the item shown? Give me numerical value as seen in the image. Adhere to instructions.\",\n            \"response\": \"\",\n            \"images\": [f\"{image_path}\"]\n        }\n        data.append(entry)\n    \n    # Write the data to a JSON file\n    with open(output_json_path, 'w') as f:\n        json.dump(data, f, indent=2)\n    \n    print(f\"Dataset prepared and saved to {output_json_path}\")\n\n# Usage\ncsv_path = '/kaggle/input/testfile/test.csv'\nimage_dir = '/kaggle/working/test_image_dir/'\noutput_json_path = '/kaggle/working/test.json'\nquestion = \"What is the value of the {row['entity_name']} of the item shown? Give me numerical value as seen in the image. Adhere to instructions.\"\nanswer_column = 'entity_value'\n\nprepare_dataset(csv_path, image_dir, output_json_path, question, answer_column)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/input/amccode/inference.py \\\n    --csv_file /kaggle/input/sample-test-amc/sample_test.csv \\\n    --start 0 \\\n    --end 88 \\\n    --chunk_size 5000 \\\n    --img_dir /kaggle/working/sample_test_image_dir \\\n    --output_csv /kaggle/working/sample_test_preds.csv","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!CUDA_VISIBLE_DEVICES=0,1 swift infer \\\n    --ckpt_dir /kaggle/working/output/qwen2-vl-2b-instruct/v3-20240914-130231/checkpoint-163 \\\n    --load_dataset_config true --merge_lora true \\\n    --dataset /kaggle/working/test.json \\\n    --dataset_test_ratio 1 \\\n\n# !CUDA_VISIBLE_DEVICES=0,1 swift infer \\\n#     --ckpt_dir /kaggle/working/output/qwen2-vl-2b-instruct/v3-20240914-130231/checkpoint-163 \\\n#     --load_args_from_ckpt_dir \\\n#     --dataset /kaggle/working/test.json \\","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/testfile/test.csv\")[:10000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-09-15T04:22:22.403639Z","iopub.execute_input":"2024-09-15T04:22:22.404337Z","iopub.status.idle":"2024-09-15T04:22:22.730652Z","shell.execute_reply.started":"2024-09-15T04:22:22.404298Z","shell.execute_reply":"2024-09-15T04:22:22.729410Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"],"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}